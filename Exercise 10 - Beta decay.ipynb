{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHR2HdwE02So"
      },
      "source": [
        "Matthew Sett\n",
        "<br>\n",
        "Date: Feb. 1, 2023\n",
        "<br>\n",
        "PHYS 2030 W23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITwabIzY02Sq"
      },
      "source": [
        "# <center><font color=#46769B>Exercise 10: Beta decay</font></center>\n",
        "\n",
        "## <font color=#46769B>Introduction</font>\n",
        "\n",
        "While neutrons found in nuclei are generally stable, a \"free\" neutron is not.\n",
        "With a half-life for decay of around 10 minutes, a lone neutron will eventually decay through the process of nuclear $\\beta$ decay.\n",
        "In this process, the neutron ($n$) is converted into a proton ($p$), emitting an electron and a neutrino in the process. A lone proton, of course, is stable, making up the nucleus of a Hydrogen atom.\n",
        "Schematically, the reaction is\n",
        "\n",
        "$$n \\to  p + e^- + \\bar{\\nu}_e \\, ,$$\n",
        "\n",
        "where $e^-$ is an electron and $\\bar{\\nu}_e$ is a *neutrino*. The neutrino is a nearly-massless particle that interacts very feebly. (To be more precise, $\\bar{\\nu}_e$ is an [electron antineutrino](https://en.wikipedia.org/wiki/Electron_neutrino). Their feebleness is why we do not perceive them readily in our day-to-day lives, despite the fact that the sun is bombarding us with trillions of neutrions every second.) \n",
        "\n",
        "Here we will concern ourselves with the readily-observable electron that is emitted. The energy of the electron is described by a continuous distribution, known as the $\\beta$-decay spectrum.<font color=red>$^1$</font>\n",
        "The $\\beta$-decay spectrum tells us the probability to observe an electron with a given energy $E$ is\n",
        "\n",
        "$$P(E) = \\left\\{ \\begin{array}{cl} A E \\sqrt{E^2 - E_m^2} (E_{\\rm max} - E)^2 & {\\rm for} \\; E_m \\le E \\le E_{\\rm max} \\\\\n",
        "0 & {\\rm otherwise} \\end{array} \\right. \\, , \\qquad (1)$$\n",
        "\n",
        "where the minimum electron energy is given by its rest mass energy $E_m = 0.511 \\; {\\rm MeV}$ and the maximum available energy is $E_{\\rm max} = 1.292 \\; {\\rm MeV}$.<font color=red>$^2$</font>  \n",
        "$A = 17.661$ is a normalizing constant.\n",
        "\n",
        "## <font color=#46769B>How good is a proposal distribution?</font>\n",
        "\n",
        "We will use __importance sampling__ to study the PDF $P(E)$ in Eq. (1) by sampling from a __proposal distribution__ $Q(E)$. We consider two choices for $Q(E)$ and we will assess which one is better. How is this done?\n",
        "\n",
        "Suppose we want to calculate the following (true) mean of a function, with respect to $P(E)$:\n",
        "\n",
        "$$\\overline{f(E)}_P = \\int_{-\\infty}^{+\\infty} dE \\, P(E) \\, f(E) \\, , \\qquad (2) $$ \n",
        "\n",
        "e.g., if we want the mean energy, we would take simply $f(E) = E$. Importance sampling says we rewrite this integral as\n",
        "\n",
        "$$\\overline{f(E)}_P = \\int_{-\\infty}^{+\\infty} dE \\, \\frac{P(E) f(E)}{Q(E)} \\, Q(E) = \\overline{\\frac{P(E) f(E)}{Q(E)}}_Q\\, . $$ \n",
        "\n",
        "Then we evaluate this integral by sampling $E$ from $Q(E)$, and then computing the mean\n",
        "\n",
        "$$\\left\\langle \\frac{P(E) f(E)}{Q(E)} \\right\\rangle = \\sum_{i=0}^{N-1} \\frac{P(E_i)}{Q(E_i)} f(E_i) = \\langle f(E) \\rangle_w$$\n",
        "\n",
        "which is the formula for the weighted mean.\n",
        "\n",
        "Now, we want our proposal distribution $Q(E)$ to give us the best approximation for the true mean. What $Q(E)$ gives us the best result? It turns out that we want $Q(E)$ to be largest where integrand in Eq. (2), $P(E) f(E)$, is largest. (Consider $f(E)$ to be a nonnegative function, for simplicity, though this is not a requirement.) Intuitively, it is easy to understand, especially if you think of each sample $E_i$ as a *measurement*. \n",
        "- If most of our measurements are where the integrand $f(E) P(E)$ gives the most contribution to the integral in (2), we will get accurate results.\n",
        "- If most of our measurements are where the integrand $f(E) P(E)$ does not contribute much to the integral, we will get less accurate results. Only a few samples, with large weights, will contribute, and effectively it's as if we had chosen a much smaller value of $N$.\n",
        "\n",
        "That is, we want $Q(E)$ to be large where $P(E) f(E)$ is large, and small where $P(E) f(E)$ is small. In fact, the optimal choice is when $Q(E)$ is *identical* to $P(E) f(E)$:\n",
        "\n",
        "$$Q(E) =  Z P(E) f(E) \\qquad (3)$$\n",
        "\n",
        "up to a normalizing constant $Z$ required by having $Q(E)$ be a valid PDF that is normalized to 1. (A proof of this can be found [here](https://stats.stackexchange.com/questions/324668/how-is-this-minimum-variance-worked-out-for-this-importance-sampling-estimator), but we will not prove it here.) Note that:\n",
        "\n",
        "- The best choice of $Q(E)$ depends not just on $P(E)$, but also on the function $f(E)$ we are calculating the mean of.\n",
        "- In practice, taking $Q(E)$ as in Eq. (3) is probably not an option. Since we assumed we couldn't sample from $P(E)$ directly, it is probably not possible to sample from Eq. (3) either!\n",
        "\n",
        "This gives us an idea for how to quantify the goodness of $Q(E)$: calculate the variance of $P(E)f(E)/Q(E)$\n",
        "\n",
        "$$\\Delta\\left(\\frac{P(E)f(E)}{Q(E)}\\right)^2 \\qquad (4)$$\n",
        "\n",
        "For the optimal choice of $Q(E)$ in (3), we have $P(E)f(E)/Q(E) = 1/Z$ for every sample, and therefore there is zero variance. The larger Eq. (4), the worse our proposal distribution.\n",
        "\n",
        "Eq. (4) is easily calculated once you have your samples `E` and your weights `w`. For the case where $f(E) = E$, we just need to evaluate\n",
        "\n",
        "```py\n",
        "numpy.var(w*E)\n",
        "```\n",
        "\n",
        "In general, if you had a different function $f(E)$, you would calculate\n",
        "\n",
        "```py\n",
        "numpy.var(w*f(E))\n",
        "```\n",
        "\n",
        "The smaller this number, the better our proposal distribution.\n",
        "\n",
        "Required reading:\n",
        "- *Lesson 4: Importance Sampling*\n",
        "\n",
        "Our goals for this notebook are:\n",
        "- Use imporance sampling to describe a target distribution.\n",
        "- Quantify a good vs bad proposal distribution\n",
        "\n",
        "### <font color=#46769B>Footnotes</font>\n",
        "\n",
        "<font color=red>$^1$</font> Historically, the continuous energy distribution of electrons in $\\beta$-decay was crucial puzzle in early particle physics. Before neutrinos were known, it was curious that the electron only possessed a *fraction* of the total available energy, seeming to violate conservation of energy. To rescue this cherished principle, Pauli postulated a new [\"little neutral\" particle](https://www.symmetrymagazine.org/article/march-2007/neutrino-invention) responsible for carrying away unseen that missing energy. \n",
        "It took several decades before neutrinos were observed directly, finally confirming Pauli's hypothesis. Fast forward to now, neutrino studies and [neutrino factories](https://www.dunescience.org/) have become an integral part of our efforts to understand the fundamental building blocks of nature.\n",
        "\n",
        "\n",
        "<font color=red>$^2$</font> We express energy in units of mega-electron-volts, where $1 \\; {\\rm MeV} = 10^6$ electron-volts $\\approx 1.6 \\times 10^{-13}\\; {\\rm Joules}$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y7PcaTm02Ss"
      },
      "source": [
        "## <font color=#46769B>Part (a)</font>\n",
        "\n",
        "Take a uniform proposal distribution $Q(E)$ that is constant in the domain $[E_m,E_{\\rm max}]$ and zero outside this domain. (Be sure your distribution is normalized correctly.) Perform the following tasks:\n",
        "\n",
        "- Generate $N = 10^5$ samples for $E$ from $Q(E)$.\n",
        "- Calculate the weights $w = P(E)/Q(E)$ from your samples.\n",
        "- To check everything, make a plot that shows:\n",
        "    - Unweighted histogram of your samples for $E$ and a plot of $Q(E)$, which should agree.\n",
        "    - Weighted histogram of your samples for $E$ and a plot of $P(E)$, which should agree.\n",
        "    - Include a legend on your plot, and choose an appropriate number of bins and opacity (`alpha`) for your histogram\n",
        "- Calculate the $\\langle E \\rangle$ and $\\Delta E/\\sqrt{N}$ with respect to $P(E)$, i.e., calculated as *weighted* means.\n",
        "- Calculate the variance of $\\frac{P(E)}{Q(E)} E$, as described above in and below Eq. (4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNyn9ntM02St"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define constants\n",
        "\n",
        "A = 17.661\n",
        "Em = 0.511\n",
        "Emax = 1.292\n",
        "\n",
        "# Your code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apMjzYFy02Su"
      },
      "source": [
        "## <font color=#46769B>Part (b)</font>\n",
        "\n",
        "Repeat the steps in Part (a) with a normal proposal distribution $Q(E) = \\mathcal{N}(\\mu,\\sigma)$. \n",
        "\n",
        "For starters, repeat these steps for the following values which yield \"bad\" proposal distributions: \n",
        "- $\\mu = 1$ and $\\sigma=100$\n",
        "- $\\mu = 1$ and $\\sigma = 0.1$\n",
        "\n",
        "Next, repeat everything again, adjusting your values of $\\mu$ and $\\sigma$ until you find a better proposal distribution than you found in Part (a). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIeOzsxy02Su"
      },
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnOA3I8f02Sv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}